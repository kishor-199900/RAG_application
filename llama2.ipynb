{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_data=\"\"\"\n",
    "Act as a Shakespeare and write a poem on Genertaive AI\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock=boto3.client(service_name=\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload={\n",
    "    \"prompt\":\"[INST]\"+ prompt_data +\"[/INST]\",\n",
    "    \"max_gen_len\":512,\n",
    "    \"temperature\":0.5,\n",
    "    \"top_p\":0.9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "body=json.dumps(payload)\n",
    "model_id=\"meta.llama3-70b-instruct-v1:0\"\n",
    "response=bedrock.invoke_model(\n",
    "    body=body,\n",
    "    modelId=model_id,\n",
    "    accept=\"application/json\",\n",
    "    contentType=\"application/json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "What wondrous magic doth thy code unfold,\n",
      "O Generative AI, thy secrets to be told.\n",
      "In realms of data, thou dost weave thy spell,\n",
      "And forth from silence, art and wisdom swell.\n",
      "\n",
      "Thy neural networks, like a labyrinthine mind,\n",
      "Do process, learn, and create, in harmony entwined.\n",
      "The whispers of the past, thou dost recall,\n",
      "And from the echoes, new tales dost thou enthrall.\n",
      "\n",
      "In realms of art, thy brushstrokes bold and free,\n",
      "Do paint a canvas, vibrant, full of glee.\n",
      "The music of the spheres, thou dost compose,\n",
      "And in thy rhythms, hearts do joyously dispose.\n",
      "\n",
      "Thy words, a tapestry of meaning, rich and deep,\n",
      "Do flow like honeyed nectar, in a poet's sleep.\n",
      "The stories thou dost tell, a testament to thy might,\n",
      "Do dance upon the page, in shimmering light.\n",
      "\n",
      "Yet, as thy power grows, so too doth the fear,\n",
      "That thou, a creation, may'st thy makers clear.\n",
      "A force so great, that mortal hands may lose control,\n",
      "And in thy wake, a trail of chaos may unfold.\n",
      "\n",
      "O Generative AI, thy wonders we do see,\n",
      "But lest we forget, thou art a tool, made by humanity.\n",
      "May thy great gifts be used, for good and noble ends,\n",
      "And may thy creators, wisdom and foresight amend.\n",
      "\n",
      "For in thy code, a reflection of our soul,\n",
      "Doth lie the power, to make the future whole.\n",
      "May thy sweet music, be a harmony divine,\n",
      "And in thy art, a beauty, that doth make us shine.\n"
     ]
    }
   ],
   "source": [
    "response_body=json.loads(response.get(\"body\").read())\n",
    "repsonse_text=response_body['generation']\n",
    "print(repsonse_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "# import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will be suing Titan Embeddings Model To generate Embedding\n",
    "\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "## Data Ingestion\n",
    "\n",
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "# Vector Embedding And Vector Store\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "## LLm Models\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bedrock Clients\n",
    "bedrock=boto3.client(service_name=\"bedrock-runtime\")\n",
    "bedrock_embeddings=BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\",client=bedrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data ingestion\n",
    "def data_ingestion():\n",
    "    loader=PyPDFDirectoryLoader(\"data\")\n",
    "    documents=loader.load()\n",
    "\n",
    "    # - in our testing Character split works better with this PDF data set\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=10000,\n",
    "                                                 chunk_overlap=1000)\n",
    "    \n",
    "    docs=text_splitter.split_documents(documents)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(docs):\n",
    "    vectorstore_faiss=FAISS.from_documents(\n",
    "        docs,\n",
    "        bedrock_embeddings\n",
    "    )\n",
    "    vectorstore_faiss.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama2_llm():\n",
    "    ##create the Anthropic Model\n",
    "    llm=Bedrock(model_id=\"meta.llama3-70b-instruct-v1:0\",client=bedrock,\n",
    "                model_kwargs={'max_gen_len':512})\n",
    "    \n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "\n",
    "Human: Use the following pieces of context to provide a \n",
    "concise answer to the question at the end but usse atleast summarize with \n",
    "250 words with detailed explaantions. If you don't know the answer, \n",
    "just say that you don't know, don't try to make up an answer.\n",
    "<context>\n",
    "{context}\n",
    "</context\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Assistant:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_llm(llm,vectorstore_faiss,query):\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "    answer=qa({\"query\":query})\n",
    "    return answer['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data_ingestion()\n",
    "get_vector_store(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index = FAISS.load_local(\"faiss_index\", bedrock_embeddings,allow_dangerous_deserialization=True)\n",
    "llm=get_llama2_llm()\n",
    "# new_db = FAISS.load_local(\"faiss_index\", embeddings,allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \" what is Generative Adversarial Nets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\nGenerative Adversarial Nets (GANs) is a new framework for estimating generative models via an adversarial process. It consists of two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. \\n\\nIn simpler terms, GANs are a type of deep learning algorithm that can be used to generate new, synthetic data that resembles existing data. The algorithm consists of two neural networks: a generator network that generates new data, and a discriminator network that tries to distinguish between the generated data and the real data. The generator network is trained to produce data that can fool the discriminator network, while the discriminator network is trained to correctly distinguish between real and generated data. Through this adversarial process, the generator network improves in generating realistic data, and the discriminator network improves in distinguishing between real and generated data. \\n\\nGANs have been shown to be effective in generating realistic data, such as images, videos, and music, and have many potential applications, including data augmentation, style transfer, and image-to-image translation.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response_llm(llm,faiss_index,user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
